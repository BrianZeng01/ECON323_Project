{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON 323 Final Project: Predicting Stress Levels through Sleep Data\n",
    "\n",
    "## Introduction\n",
    "In this project, we will aim to predict prices of airbnbs in NYC. As suggested in our project feedback, we will be making minimal use of existing package models. We hope to further explore the space of Machine Learning by instead coding our own models and using methods beyond the scope of this course. We were inspired to do price predictions as one of us was recently looking at airbnbs in NYC for a trip in the summer and surprisingly there was a dataset on this exact information. **Note: we pivoted from our inital sleep study test because after further investigation it seems it was an auto-generated dataset with no interesting findings.**\n",
    "\n",
    "Below is a high-level outline of our procedure\n",
    "1. Pre-process the data\n",
    "2. Do some statstical analysis on features to see if there are any outliers and adjust accordingly\n",
    "3. Reserve 20% of the dataset only to be used for testing once at the end\n",
    "4. Use bootstraping to improve training phase\n",
    "5. Build a random forest from scratch incrementally and display results throughout.\n",
    "6. Present our results and interpretations numerically/visually (eg. training visualizations). \n",
    "7. Compare with common models from popular packages\n",
    "7. Discussion about the shortcomings of our model and/or experimentation process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patsy in c:\\users\\brian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\brian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from patsy) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.4 in c:\\users\\brian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from patsy) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install patsy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, ensemble, linear_model, utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be utilizing a dataset with [New York City Airbnb Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv) as seen below. We will first do some preprocessing of the data to make it model compatible such as removing some columns and categorizing our target label \"price\". We will be categorizing it to approach it with classification methods and because price categories are intuitively reasonable. Then we will shuffle and seperate the dataset to our test and training sets.\n",
    "\n",
    "Acknowledgements\n",
    "1. L. Rachakonda, A. K. Bapatla, S. P. Mohanty, and E. Kougianos, “SaYoPillow: Blockchain-Integrated Privacy-Assured IoMT Framework for Stress Management Considering Sleeping Habits”, IEEE Transactions on Consumer Electronics (TCE), Vol. 67, No. 1, Feb 2021, pp. 20-29.\n",
    "2. L. Rachakonda, S. P. Mohanty, E. Kougianos, K. Karunakaran, and M. Ganapathiraju, “Smart-Pillow: An IoT based Device for Stress Detection Considering Sleeping Habits”, in Proceedings of the 4th IEEE International Symposium on Smart Electronic Systems (iSES), 2018, pp. 161--166."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15184\n",
      "209.55677028451\n",
      "[100, 300, 500, 700]\n",
      "       neighbourhood_group  neighbourhood  latitude  longitude  room_type  \\\n",
      "15058                    0             17  40.76647  -73.98290          0   \n",
      "30000                    0              5  40.72939  -73.98553          0   \n",
      "22700                    0             29  40.76989  -73.98229          0   \n",
      "2170                     0             29  40.79962  -73.96523          1   \n",
      "13326                    0              6  40.70623  -74.00906          1   \n",
      "\n",
      "       price  minimum_nights  number_of_reviews  reviews_per_month  \\\n",
      "15058      1              30                  0               0.00   \n",
      "30000      3               4                  5               0.30   \n",
      "22700      0              30                  4               0.16   \n",
      "2170       0               7                 70               0.93   \n",
      "13326      3               1                  0               0.00   \n",
      "\n",
      "       calculated_host_listings_count  availability_365  \n",
      "15058                             121               345  \n",
      "30000                               1                 0  \n",
      "22700                               1               124  \n",
      "2170                                1                44  \n",
      "13326                               1                 0  \n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"AB_NYC_2019.csv\")\n",
    "df = df[df[\"neighbourhood_group\"] == \"Manhattan\"]\n",
    "df = df[(df[\"price\"] >= 100) & (df[\"price\"] <= 700)]\n",
    "print(len(df))\n",
    "print(np.mean(df[\"price\"]))\n",
    "df = df.sample(n=5000) # only take 2000 otherwise takes too long\n",
    "df = df.drop([\"id\", \"name\" , \"host_id\", \"host_name\", \"last_review\"], axis=1)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# categorize price ranges\n",
    "# bins_25 = np.array([i*50 for i in range(0,11)]) # bins of 25 from 0 to 500\n",
    "# bins_50 = np.array([i*100 for i in range(6,16)]) # bins of 100 from 500 to 2000\n",
    "# bins_100 = np.array([i*250 for i in range(7,41)]) # bins of 250 from 2000 to 10000\n",
    "# bins = np.concatenate((bins_25, bins_50, bins_100))\n",
    "\n",
    "# bins_25 = np.array([i*25 for i in range(0,21)]) # bins of 25 from 0 to 500\n",
    "# bins_50 = np.array([i*50 for i in range(11,41)]) # bins of 100 from 500 to 2000\n",
    "# bins_100 = np.array([i*100 for i in range(21,101)]) # bins of 250 from 2000 to 10000\n",
    "# bins = np.concatenate((bins_25, bins_50, bins_100))\n",
    "\n",
    "bins = [100,300,500,700]\n",
    "print(bins)\n",
    "labels = np.array(range(len(bins)-1))\n",
    "class_count = len(labels)\n",
    "df['price'] = pd.cut(df['price'], bins=bins, labels=labels)\n",
    "\n",
    "# give labels to string fields\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[\"neighbourhood_group\"])\n",
    "df[\"neighbourhood_group\"] = le.transform(df[\"neighbourhood_group\"])\n",
    "le.fit(df[\"neighbourhood\"])\n",
    "df[\"neighbourhood\"] = le.transform(df[\"neighbourhood\"])\n",
    "le.fit(df[\"room_type\"])\n",
    "df[\"room_type\"] = le.transform(df[\"room_type\"])\n",
    "le.fit(df[\"room_type\"])\n",
    "df[\"room_type\"] = le.transform(df[\"room_type\"])\n",
    "le.fit(df[\"price\"])\n",
    "df[\"price\"] = le.transform(df[\"price\"])\n",
    "\n",
    "print(df.head())\n",
    "print(len(df))\n",
    "\n",
    "num_training = int(len(df)*0.8)\n",
    "X = df.drop([\"price\"], axis=1)\n",
    "y = df[\"price\"]\n",
    "X, y = utils.shuffle(X, y, random_state=42)\n",
    "n, d = X.shape\n",
    "X_train = X.iloc[:num_training,:].values\n",
    "X_test = X.iloc[num_training:,:].values\n",
    "y_train = y.iloc[:num_training].values\n",
    "y_test = y.iloc[num_training:].values\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "We will cull any data that seems to be out of a reasonable range here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now bootstrap the training data, during training we will train on each sampled bootstrap and take the mode of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstraps = 5\n",
    "bootstrap_samples = np.zeros((num_bootstraps, num_training, X.shape[1]))\n",
    "bootstrap_labels = np.zeros((num_bootstraps, num_training)) \n",
    "for i in range(num_bootstraps):\n",
    "    # randomly sample the dataset with replacement\n",
    "    bootstrap_indices = np.random.choice(num_training, num_training, replace=True)\n",
    "    bootstrap_samples[i] = X_train[bootstrap_indices]\n",
    "    bootstrap_labels[i] = y_train[bootstrap_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Decision Stump\n",
    "We will use the most common splitting criterion of information gain which aims to select the feature that reduces entropy. This will be the most atomic unit that we construct our Decision Tree from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, mode\n",
    "\n",
    "class DecisionStump:\n",
    "    y_hat_yes = None\n",
    "    y_hat_no = None\n",
    "    j_best = None\n",
    "    t_best = None\n",
    "\n",
    "    def getProbs(self, n, classCount, count):\n",
    "        if(n == 0): return []\n",
    "        p = np.zeros(classCount)\n",
    "        for i in range(classCount):\n",
    "            p[i] = count[i]/n\n",
    "\n",
    "        return p\n",
    "    \n",
    "    def mode(self, y_subset):\n",
    "        if(len(y_subset) == 0): return -1\n",
    "        return np.argmax(np.bincount(y_subset))\n",
    "\n",
    "    def fit(self, X, y, feature_space=None):\n",
    "        n, d = X.shape\n",
    "\n",
    "        # Get an array with the number of 0's, number of 1's, etc.\n",
    "        count = np.bincount(y, minlength=class_count)\n",
    "\n",
    "        # Get the index of the largest value in count.\n",
    "        # Thus, y_mode is the mode (most popular value) of y\n",
    "        y_mode = np.argmax(count)\n",
    "\n",
    "        self.y_hat_yes = y_mode\n",
    "\n",
    "        # If all the labels are the same, no need to split further\n",
    "        if np.unique(y).size <= 1:\n",
    "            return\n",
    "\n",
    "        # Get current entropy\n",
    "        p = self.getProbs(n, class_count, count)\n",
    "        ent = entropy(p)\n",
    "        best_info_gain = 0\n",
    "        \n",
    "        if(feature_space is None):\n",
    "            feature_space = range(d)\n",
    "\n",
    "        # Loop over features looking for the best split\n",
    "        for j in feature_space:\n",
    "            for i in range(n):\n",
    "                # Choose value for threshold \n",
    "                t = X[i,j] \n",
    "\n",
    "                yes_subset = y[X[:,j] > t]\n",
    "                no_subset = y[X[:,j] <= t]\n",
    "\n",
    "                # find best prediction for each side of the split\n",
    "                y_yes_mode = self.mode(yes_subset)\n",
    "                y_no_mode = self.mode(no_subset)\n",
    "\n",
    "                # compute the entropy of each side of the split\n",
    "                yes_counts = np.bincount(yes_subset, minlength=class_count)\n",
    "                no_counts = np.bincount(no_subset, minlength=class_count)\n",
    "                yes_p = self.getProbs(len(yes_subset), class_count, yes_counts)\n",
    "                no_p = self.getProbs(len(no_subset), class_count, no_counts)\n",
    "\n",
    "                ent_yes = (len(yes_subset)/n)*entropy(yes_p) \n",
    "                ent_no = (len(no_subset)/n)*entropy(no_p)\n",
    "                weighted_entropy = ent_yes + ent_no\n",
    "\n",
    "                # calculate info gain from the splits when weighted\n",
    "                information_gain = ent - weighted_entropy\n",
    "\n",
    "                # Compare to information gain so far\n",
    "                if information_gain > best_info_gain:\n",
    "                    best_info_gain = information_gain\n",
    "                    self.j_best = j\n",
    "                    self.t_best = t\n",
    "                    self.y_hat_yes = y_yes_mode\n",
    "                    self.y_hat_no = y_no_mode   \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if self.j_best is None:\n",
    "            return self.y_hat_yes * np.ones(n)\n",
    "\n",
    "        y_hat = np.zeros(n)\n",
    "\n",
    "        # assign predictions\n",
    "        for i in range(n):\n",
    "            if X[i, self.j_best] > self.t_best:\n",
    "                y_hat[i] = self.y_hat_yes\n",
    "            else:\n",
    "                y_hat[i] = self.y_hat_no\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make minor modifications to create a random stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStump(DecisionStump):\n",
    "    def fit(self, X, y):\n",
    "        d = X.shape[1]\n",
    "        sqrt_d = int(np.floor(np.sqrt(d)))\n",
    "\n",
    "        chosen_features = np.random.choice(d, sqrt_d, replace=False)\n",
    "\n",
    "        DecisionStump.fit(self, X, y, feature_space=chosen_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Decision Tree\n",
    "We will now create a decision tree by recursively splitting at each stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    stump = None\n",
    "    yes_path = None\n",
    "    no_path = None\n",
    "\n",
    "    def __init__(self, max_depth, random=False):\n",
    "        self.max_depth = max_depth\n",
    "        self.random = random\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        stump = RandomStump() if self.random else DecisionStump()\n",
    "        stump.fit(X, y)\n",
    "\n",
    "        if self.max_depth <= 1 or stump.j_best is None: # max depth leaf stump or no point in splitting\n",
    "            self.stump = stump\n",
    "            self.yes_path = None\n",
    "            self.no_path = None\n",
    "            return\n",
    "\n",
    "        j = stump.j_best\n",
    "        value = stump.t_best\n",
    "\n",
    "        yes = X[:, j] > value\n",
    "        no = X[:, j] <= value\n",
    "\n",
    "        # recursively build tree\n",
    "        self.stump = stump\n",
    "        self.yes_path = DecisionTree(self.max_depth - 1)\n",
    "        self.yes_path.fit(X[yes], y[yes])\n",
    "        self.no_path = DecisionTree(self.max_depth - 1)\n",
    "        self.no_path.fit(X[no], y[no])\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, d = X.shape\n",
    "        y = np.zeros(n)\n",
    "\n",
    "\n",
    "        if self.stump.j_best is None or self.yes_path is None:  # just use this stump\n",
    "            y = self.stump.predict(X)\n",
    "        else:\n",
    "            j = self.stump.j_best\n",
    "            t = self.stump.t_best\n",
    "\n",
    "            yes = X[:, j] > t\n",
    "            no = X[:, j] <= t\n",
    "\n",
    "            y[yes] = self.yes_path.predict(X[yes])\n",
    "            y[no] = self.no_path.predict(X[no])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing the Decision Tree\n",
    "Let us see what validation error we can achieve with a single decision tree with various depths while making use of k fold validation with k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_entropy.py:135: RuntimeWarning: invalid value encountered in divide\n",
      "  pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:5, validation accuracy: 0.8054984135839057\n",
      "depth:7, validation accuracy: 0.7962487248498782\n",
      "depth:10, validation accuracy: 0.7799987853035223\n"
     ]
    }
   ],
   "source": [
    "depths = [5,7,10]\n",
    "\n",
    "def k_fold_validation(model, X, y, k=4):\n",
    "    total_accuracy = 0\n",
    "    for i in range (1,k+1):\n",
    "        left = int(len(X)*((i-1)*(1/k)))\n",
    "        right = int(len(X)*((i)*(1/k)))\n",
    "        X_fold_train = np.concatenate([X[:left],X[right:]])\n",
    "        X_fold_test = X[left:right]\n",
    "        y_fold_train = np.concatenate([y[:left],y[right:]])\n",
    "        y_fold_test = y[left:right]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_hat_test = model.predict(X_fold_test)\n",
    "        err_test = np.mean(y_hat_test != y_fold_test)\n",
    "        accuracy = 1 - err_test\n",
    "        total_accuracy += accuracy\n",
    "    \n",
    "    return total_accuracy / k\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTree(depth)\n",
    "    validation_acc = k_fold_validation(model, X_train, y_train, k=3)\n",
    "    print(f\"depth:{depth}, validation accuracy: {validation_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above increasing depth from 5 to 10 increases our validation accuracy but from 10 to 15 it drops likely due to overfitting with increased depth. Regardless, it is at a very slow rate and we are still getting quite a poor prediction so lets move on\n",
    "\n",
    "## Random Tree\n",
    "We will generate a random tree which differs from the decision tree in that it bootstraps the passed in X and y and the stumps will only be applied to $\\sqrt{d}$ features. This can simply be done with a few modifications to our original tree and stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTree(DecisionTree):\n",
    "    def __init__(self, max_depth):\n",
    "        DecisionTree.__init__(self, max_depth=max_depth, random=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        boostrap_indices = np.random.choice(n, n, replace=True)\n",
    "        bootstrap_X = X[boostrap_indices]\n",
    "        bootstrap_y = y[boostrap_indices]\n",
    "\n",
    "        DecisionTree.fit(self, bootstrap_X, bootstrap_y)\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    num_trees = 0\n",
    "    max_depth = 0\n",
    "    trees = []\n",
    "\n",
    "    def __init__(self, num_trees, max_depth):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.num_trees):\n",
    "            tree = RandomTree(self.max_depth)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        for i in range(len(self.trees)):\n",
    "            curr_pred = self.trees[i].predict(X)\n",
    "            if(i == 0):\n",
    "                y_pred = curr_pred\n",
    "            else: \n",
    "                np.vstack((y_pred, curr_pred))\n",
    "        \n",
    "        return mode(y_pred)[0].flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a random forest with 5 random trees see our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_26388\\2441632914.py:38: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(y_pred)[0].flatten()\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForest(5, 10)\n",
    "validation_acc = k_fold_validation(random_forest, X_train, y_train, k=4)\n",
    "print(f\"validation accuracy: {validation_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_26388\\2441632914.py:38: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(y_pred)[0].flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.28583333333333333\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForest(5, 10)\n",
    "validation_acc = k_fold_validation(random_forest, X_train, y_train, k=4)\n",
    "print(f\"validation accuracy: {validation_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n",
      "0\n",
      "0.415\n",
      "0.0\n",
      "0.415\n",
      "0.28583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_26388\\1647491162.py:18: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(np.count_nonzero(y_train == y_hat)/len(y_hat))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=5, max_depth=5)\n",
    "forest.fit(X_train,y_train)\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_hat_lr = lr.predict(X_train)\n",
    "y_hat = forest.predict(X_train)\n",
    "print(np.count_nonzero(y_train == y_hat)/len(y_hat))\n",
    "print(np.count_nonzero(y_train == y_hat_lr))\n",
    "\n",
    "y_test_hat = forest.predict(X_test)\n",
    "print(np.count_nonzero(y_test == y_test_hat)/len(y_test))\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_hat = nb.predict(X_test)\n",
    "print(np.count_nonzero(y_train == y_hat)/len(y_hat))\n",
    "print(np.count_nonzero(y_test == y_test_hat)/len(y_test))\n",
    "\n",
    "print(len(y_train[y_train == 1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
