{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON 323 Final Project: Predicting Price Ranges of Airbnb's in NYC\n",
    "\n",
    "Written by:\n",
    "- Brian Zeng (38082186) (section 004)\n",
    "- Joyce Lu (69331106) (section 004)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this project, we will aim to predict price ranges of airbnbs in NYC. As suggested in our project feedback, we will be making minimal use of existing package models. We hope to further explore the space of Machine Learning by instead coding our own random forest and using methods beyond the scope of this course. We have decided to use a random forest as it is one of the best out of the box classification methods. Other methods we considered were Naive Bayes and KNN, but random forests also provide a more incremental approach which is desirable for the purpose of this project. We were inspired to do price predictions as one of us was recently looking at airbnbs in Manhattan for a trip in the summer and surprisingly there was a dataset on this exact information. **Note: we pivoted from our inital sleep study test because after further investigation it seems it was an auto-generated dataset with no interesting findings.**\n",
    "\n",
    "Below is a high-level outline of our procedure\n",
    "1. Pre-process the data\n",
    "2. Create test and training sets\n",
    "3. Build a normal decision tree\n",
    "4. Extend the normal decision trees to create random trees \n",
    "5. Analyze our results \n",
    "6. Discussion about the shortcomings of our model and/or experimentation process\n",
    "7. Compare with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be utilizing a dataset with [New York City Airbnb Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv). We will first do some preprocessing of the data to make it model compatible such as removing some columns and categorizing our target label \"price\". We will be categorizing it to approach it with classification methods and because price categories are intuitively reasonable. To simplify things we will also only look at Manhattan airbnbs from 100 to 500 dollars. Then we will shuffle and seperate the dataset to our test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       neighbourhood_group  neighbourhood  latitude  longitude  room_type  \\\n",
      "7679                     0             17  40.75245  -73.97342          1   \n",
      "1319                     0              5  40.73047  -73.98229          0   \n",
      "540                      0             11  40.76415  -73.99067          0   \n",
      "6466                     0             28  40.77429  -73.95628          0   \n",
      "24798                    0             29  40.77476  -73.98947          0   \n",
      "\n",
      "       price  minimum_nights  number_of_reviews  reviews_per_month  \\\n",
      "7679       3               1                  6               0.12   \n",
      "1319       8               5                  6               0.07   \n",
      "540        0               1                122               1.29   \n",
      "6466       2               7                  8               0.16   \n",
      "24798      4              30                  8               0.38   \n",
      "\n",
      "       calculated_host_listings_count  availability_365  \n",
      "7679                                3                 0  \n",
      "1319                                1                 0  \n",
      "540                                 1                20  \n",
      "6466                                1                 0  \n",
      "24798                              25               261  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"AB_NYC_2019.csv\")\n",
    "df = df[df[\"neighbourhood_group\"] == \"Manhattan\"]\n",
    "df = df[(df[\"price\"] >= 100) & (df[\"price\"] <= 700)]\n",
    "df = df.sample(n=2500) # only take 2500 otherwise takes too long\n",
    "df = df.drop([\"id\", \"name\" , \"host_id\", \"host_name\", \"last_review\"], axis=1)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# categorize price ranges\n",
    "bins = [100,150,200,250,300,350,400,450,500]\n",
    "labels = np.array(range(len(bins)-1))\n",
    "class_count = len(labels)\n",
    "df['price'] = pd.cut(df['price'], bins=bins, labels=labels)\n",
    "\n",
    "# give labels to string fields\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[\"neighbourhood_group\"])\n",
    "df[\"neighbourhood_group\"] = le.transform(df[\"neighbourhood_group\"])\n",
    "le.fit(df[\"neighbourhood\"])\n",
    "df[\"neighbourhood\"] = le.transform(df[\"neighbourhood\"])\n",
    "le.fit(df[\"room_type\"])\n",
    "df[\"room_type\"] = le.transform(df[\"room_type\"])\n",
    "le.fit(df[\"room_type\"])\n",
    "df[\"room_type\"] = le.transform(df[\"room_type\"])\n",
    "le.fit(df[\"price\"])\n",
    "df[\"price\"] = le.transform(df[\"price\"])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "num_training = int(len(df)*0.8)\n",
    "X = df.drop([\"price\"], axis=1)\n",
    "y = df[\"price\"]\n",
    "X, y = utils.shuffle(X, y, random_state=42)\n",
    "n, d = X.shape\n",
    "X_train = X.iloc[:num_training,:].values\n",
    "X_test = X.iloc[num_training:,:].values\n",
    "y_train = y.iloc[:num_training].values\n",
    "y_test = y.iloc[num_training:].values\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "This is our baseline training and test accuracy which is just picking the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline training accuracy: 0.314\n",
      "baseline test accuracy: 0.316\n"
     ]
    }
   ],
   "source": [
    "baseline_train = len(y_train[y_train == np.argmax(np.bincount(y_train))])/len(y_train) \n",
    "baseline_test = len(y_test[y_test == np.argmax(np.bincount(y_test))])/len(y_test) \n",
    "print(f\"baseline training accuracy: {baseline_train}\")\n",
    "print(f\"baseline test accuracy: {baseline_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Decision Stump\n",
    "We will use the most common splitting criterion of information gain which aims to select the feature that reduces entropy. This will be the most atomic unit that we construct our Decision Tree from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, mode\n",
    "\n",
    "class DecisionStump:\n",
    "    y_hat_yes = None\n",
    "    y_hat_no = None\n",
    "    j_best = None\n",
    "    t_best = None\n",
    "\n",
    "    def getProbs(self, n, classCount, count):\n",
    "        if(n == 0): return []\n",
    "        p = np.zeros(classCount)\n",
    "        for i in range(classCount):\n",
    "            p[i] = count[i]/n\n",
    "\n",
    "        p[p == 0] = 1e-9 # fixes divide by 0 error in entropy function\n",
    "        return p\n",
    "    \n",
    "    def mode(self, y_subset):\n",
    "        if(len(y_subset) == 0): return -1\n",
    "        return np.argmax(np.bincount(y_subset))\n",
    "\n",
    "    def fit(self, X, y, feature_space=None):\n",
    "        n, d = X.shape\n",
    "\n",
    "        # Get an array with the number of 0's, number of 1's, etc.\n",
    "        count = np.bincount(y, minlength=class_count)\n",
    "\n",
    "        # Get the index of the largest value in count.\n",
    "        # Thus, y_mode is the mode (most popular value) of y\n",
    "        y_mode = np.argmax(count)\n",
    "\n",
    "        self.y_hat_yes = y_mode\n",
    "\n",
    "        # If all the labels are the same, no need to split further\n",
    "        if np.unique(y).size <= 1:\n",
    "            return\n",
    "\n",
    "        # Get current entropy\n",
    "        p = self.getProbs(n, class_count, count)\n",
    "        ent = entropy(p)\n",
    "        best_info_gain = 0\n",
    "        \n",
    "        if(feature_space is None):\n",
    "            feature_space = range(d)\n",
    "\n",
    "        # Loop over features looking for the best split\n",
    "        for j in feature_space:\n",
    "            for i in range(n):\n",
    "                # Choose value for threshold \n",
    "                t = X[i,j] \n",
    "\n",
    "                yes_subset = y[X[:,j] > t]\n",
    "                no_subset = y[X[:,j] <= t]\n",
    "\n",
    "                # find best prediction for each side of the split\n",
    "                y_yes_mode = self.mode(yes_subset)\n",
    "                y_no_mode = self.mode(no_subset)\n",
    "\n",
    "                # compute the entropy of each side of the split\n",
    "                yes_counts = np.bincount(yes_subset, minlength=class_count)\n",
    "                no_counts = np.bincount(no_subset, minlength=class_count)\n",
    "                yes_p = self.getProbs(len(yes_subset), class_count, yes_counts)\n",
    "                no_p = self.getProbs(len(no_subset), class_count, no_counts)\n",
    "\n",
    "                ent_yes = (len(yes_subset)/n)*entropy(yes_p) \n",
    "                ent_no = (len(no_subset)/n)*entropy(no_p)\n",
    "                weighted_entropy = ent_yes + ent_no\n",
    "\n",
    "                # calculate info gain from the splits when weighted\n",
    "                information_gain = ent - weighted_entropy\n",
    "\n",
    "                # Compare to information gain so far\n",
    "                if information_gain > best_info_gain:\n",
    "                    best_info_gain = information_gain\n",
    "                    self.j_best = j\n",
    "                    self.t_best = t\n",
    "                    self.y_hat_yes = y_yes_mode\n",
    "                    self.y_hat_no = y_no_mode   \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if self.j_best is None:\n",
    "            return self.y_hat_yes * np.ones(n)\n",
    "\n",
    "        y_hat = np.zeros(n)\n",
    "\n",
    "        # assign predictions\n",
    "        for i in range(n):\n",
    "            if X[i, self.j_best] > self.t_best:\n",
    "                y_hat[i] = self.y_hat_yes\n",
    "            else:\n",
    "                y_hat[i] = self.y_hat_no\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make minor modifications to create a random stump, the stumps will only be applied to $\\sqrt{d}$ features to avoid the same trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStump(DecisionStump):\n",
    "    def fit(self, X, y):\n",
    "        d = X.shape[1]\n",
    "        sqrt_d = int(np.floor(np.sqrt(d)))\n",
    "\n",
    "        chosen_features = np.random.choice(d, sqrt_d, replace=False)\n",
    "\n",
    "        DecisionStump.fit(self, X, y, feature_space=chosen_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Decision Tree\n",
    "We will now create a decision tree by recursively splitting at each stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    stump = None\n",
    "    yes_path = None\n",
    "    no_path = None\n",
    "\n",
    "    def __init__(self, max_depth, random=False):\n",
    "        self.max_depth = max_depth\n",
    "        self.random = random\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        stump = RandomStump() if self.random else DecisionStump()\n",
    "        stump.fit(X, y)\n",
    "\n",
    "        if self.max_depth <= 1 or stump.j_best is None: # max depth leaf stump or no point in splitting\n",
    "            self.stump = stump\n",
    "            self.yes_path = None\n",
    "            self.no_path = None\n",
    "            return\n",
    "\n",
    "        j = stump.j_best\n",
    "        value = stump.t_best\n",
    "\n",
    "        yes = X[:, j] > value\n",
    "        no = X[:, j] <= value\n",
    "\n",
    "        # recursively build tree\n",
    "        self.stump = stump\n",
    "        self.yes_path = DecisionTree(self.max_depth - 1)\n",
    "        self.yes_path.fit(X[yes], y[yes])\n",
    "        self.no_path = DecisionTree(self.max_depth - 1)\n",
    "        self.no_path.fit(X[no], y[no])\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, d = X.shape\n",
    "        y = np.zeros(n)\n",
    "\n",
    "\n",
    "        if self.stump.j_best is None or self.yes_path is None:  # just use this stump\n",
    "            y = self.stump.predict(X)\n",
    "        else:\n",
    "            j = self.stump.j_best\n",
    "            t = self.stump.t_best\n",
    "\n",
    "            yes = X[:, j] > t\n",
    "            no = X[:, j] <= t\n",
    "\n",
    "            y[yes] = self.yes_path.predict(X[yes])\n",
    "            y[no] = self.no_path.predict(X[no])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing the Decision Tree\n",
    "Let us see what validation error we can achieve with a single decision tree with various depths while making use of k fold validation with k = 4. Utilizing k fold validation will reduce overfitting as we take the mean of multiple folds of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:3, validation accuracy: 0.337, training accuracy: 0.374\n",
      "depth:5, validation accuracy: 0.342, training accuracy: 0.408\n",
      "depth:10, validation accuracy: 0.313, training accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "depths = [3,5,10]\n",
    "\n",
    "def k_fold_validation(model, X, y, k=4):\n",
    "    total_val_acc = 0\n",
    "    total_train_acc = 0\n",
    "    for i in range (1,k+1):\n",
    "        left = int(len(X)*((i-1)*(1/k)))\n",
    "        right = int(len(X)*((i)*(1/k)))\n",
    "        X_fold_train = np.concatenate([X[:left],X[right:]])\n",
    "        X_fold_test = X[left:right]\n",
    "        y_fold_train = np.concatenate([y[:left],y[right:]])\n",
    "        y_fold_test = y[left:right]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_hat_test = model.predict(X_fold_test)\n",
    "        y_hat_train = model.predict(X_fold_train)\n",
    "        err_test = np.mean(y_hat_test != y_fold_test)\n",
    "        err_train = np.mean(y_hat_train != y_fold_train)\n",
    "        val_acc = 1 - err_test\n",
    "        train_acc = 1 - err_train\n",
    "        total_val_acc += val_acc\n",
    "        total_train_acc += train_acc\n",
    "    \n",
    "    return round(total_val_acc / k, 3), round(total_train_acc / k, 3)\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTree(depth)\n",
    "    val_acc, train_acc = k_fold_validation(model, X_train, y_train, k=3)\n",
    "    print(f\"depth:{depth}, validation accuracy: {val_acc}, training accuracy: {train_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great illustration of the fundamental trade-off. As expected, increasing depth increases training accuracy as we simply capture more of the training set. However, our validation accuracy increases from 3 to 5 but decreases from 5 to 10. This is due to overfitting with increased depth. Lets move on and see how we can improve using a random forest.\n",
    "\n",
    "## Random Tree\n",
    "We will generate a random tree which differs from the decision tree in that it bootstraps the passed in X and y to better generalize. Our random forest will simply be a collection of random trees where we predict on the mode of all of their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTree(DecisionTree):\n",
    "    def __init__(self, max_depth):\n",
    "        DecisionTree.__init__(self, max_depth=max_depth, random=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        boostrap_indices = np.random.choice(n, n, replace=True)\n",
    "        bootstrap_X = X[boostrap_indices]\n",
    "        bootstrap_y = y[boostrap_indices]\n",
    "\n",
    "        DecisionTree.fit(self, bootstrap_X, bootstrap_y)\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    num_trees = 0\n",
    "    max_depth = 0\n",
    "    trees = []\n",
    "\n",
    "    def __init__(self, num_trees, max_depth):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.num_trees):\n",
    "            tree = RandomTree(self.max_depth)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        for i in range(len(self.trees)):\n",
    "            curr_pred = self.trees[i].predict(X)\n",
    "            if(i == 0):\n",
    "                y_pred = curr_pred\n",
    "            else: \n",
    "                y_pred = np.vstack((y_pred, curr_pred))\n",
    "        \n",
    "        return mode(y_pred)[0].flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a couple random forests to tune the hyperparameters of depth and number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_26388\\2089471679.py:38: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(y_pred)[0].flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1, trees: 1, validation accuracy: 0.316, training accuracy: 0.307\n",
      "depth: 3, trees: 1, validation accuracy: 0.348, training accuracy: 0.349\n",
      "depth: 5, trees: 1, validation accuracy: 0.374, training accuracy: 0.376\n",
      "depth: 10, trees: 1, validation accuracy: 0.403, training accuracy: 0.409\n",
      "depth: 1, trees: 3, validation accuracy: 0.385, training accuracy: 0.389\n",
      "depth: 3, trees: 3, validation accuracy: 0.394, training accuracy: 0.39\n",
      "depth: 5, trees: 3, validation accuracy: 0.392, training accuracy: 0.395\n",
      "depth: 10, trees: 3, validation accuracy: 0.408, training accuracy: 0.416\n",
      "depth: 1, trees: 5, validation accuracy: 0.424, training accuracy: 0.426\n",
      "depth: 3, trees: 5, validation accuracy: 0.409, training accuracy: 0.404\n",
      "depth: 5, trees: 5, validation accuracy: 0.384, training accuracy: 0.386\n",
      "depth: 10, trees: 5, validation accuracy: 0.407, training accuracy: 0.413\n",
      "depth: 1, trees: 10, validation accuracy: 0.407, training accuracy: 0.41\n",
      "depth: 3, trees: 10, validation accuracy: 0.389, training accuracy: 0.388\n",
      "depth: 5, trees: 10, validation accuracy: 0.381, training accuracy: 0.382\n",
      "depth: 10, trees: 10, validation accuracy: 0.393, training accuracy: 0.395\n",
      "Best validation accuracy: 0.424, depth = 1, best num_trees = 5\n"
     ]
    }
   ],
   "source": [
    "best_num_trees = 0\n",
    "best_depth = 0\n",
    "best_acc = 0\n",
    "forest_sizes = [1,3,5,10]\n",
    "depths = [1,3,5,10]\n",
    "\n",
    "for num_trees in forest_sizes:\n",
    "    for depth in depths:\n",
    "        random_forest = RandomForest(max_depth=depth, num_trees=num_trees)\n",
    "        val_acc, train_acc = k_fold_validation(random_forest, X_train, y_train, k=4)\n",
    "        print(f\"depth: {depth}, trees: {num_trees}, validation accuracy: {val_acc}, training accuracy: {train_acc}\")\n",
    "        if(val_acc > best_acc):\n",
    "            best_acc = val_acc\n",
    "            best_depth = depth\n",
    "            best_num_trees = num_trees\n",
    "\n",
    "print(f\"Best validation accuracy: {best_acc}, depth = {best_depth}, best num_trees = {best_num_trees}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the single decision tree, increased depth did not give a significantly higher training accuracy as random trees and random forests reduce the amount of overfitting.\n",
    "\n",
    "Now that we have the best hyperparameters that we could find, we can run this model on our test set and see what our final test accuracy will be. Ideally it should be very close to the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.396 with depth=1 and num_trees=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Temp\\ipykernel_26388\\2089471679.py:38: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(y_pred)[0].flatten()\n"
     ]
    }
   ],
   "source": [
    "model = RandomForest(max_depth=best_depth, num_trees=best_num_trees)\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "err_test = np.mean(y_hat_test != y_test)\n",
    "test_acc = round(1 - err_test, 3)\n",
    "print(f\"Test accuracy: {test_acc} with depth={best_depth} and num_trees={best_num_trees}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test accuracy of 0.396 is not far off from our validation and training error for that respective forest which is good to see. This is approximately a 0.08 increase in accuracy in comparison to baseline, so not great but still an improvement. With that, the conclusion we can draw is that Manhattan airbnb prices are absolutely unreasonable and make no sense. This conclusion may or may not be biased by the $2000 dollar bill one had to pay for 5 nights...\n",
    "\n",
    "## Discussion and Conclusion\n",
    "In all seriousness, there may be many reasons for the poor prediction score. The lack of compute resulted in using a small subset of the data with very limited hyperparameter tuning. The dataset itself could also be lacking very important features when it comes to price such as amenities, square footage, etc. If we could improve our procedure we would do more rigourous hyperparameter tuning as well as using more of the dataset. Focusing on the data, perhaps the bins we chose were not good ranges, but this is hard to tell without overfitting bins to the training set. Statistical analysis and removal of outliers would also be a good idea in the pre-processing stage, however we agreed it would out of scope for this project. Overall the development of the random forest model was a great learning experience (aside from the frustrating debugging after waiting for the model to train)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Aside) Pre-built Random Forest\n",
    "Lastly, lets take a look at what results we get from sklearn. The random forest from the sklearn package seems to perform similarly poorly which gives confidence that our random forest was coded properly. Surprisingly ours does perform better, though without looking through the exact implementation of sklearn it would be hard to tell why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.384 training accuracy: 0.356\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=5, max_depth=1, criterion=\"entropy\")\n",
    "forest.fit(X_train,y_train)\n",
    "y_hat = forest.predict(X_train)\n",
    "y_test_hat = forest.predict(X_test)\n",
    "train_acc = round(1 - np.mean(y_hat != y_train), 3)\n",
    "test_acc = round(1 - np.mean(y_test_hat != y_test), 3)\n",
    "print(f\"Test accuracy: {test_acc} training accuracy: {train_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
